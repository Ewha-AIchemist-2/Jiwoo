# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nVxEcBMKMPF2mFsunj_LFnYPiAtZ_QuG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib

cust_df = pd.read_csv("/content/train.csv", encoding='latin-1')
cust_df = pd.read_csv("/content/sample_submission.csv", encoding='latin-1')
cust_df = pd.read_csv("/content/test.csv", encoding='latin-1')

cust_df.shape
cust_df.info()
cust_df.describe()

cust_df['var3'].value_counts()

cust_df['var3'].replace(-99999, 2, inplace=True)

cust_df.drop('ID', axis=1, inplace=True)

X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.iloc[:, -1]

from sklearn.model_selection import train_test_split

## train, test 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(
    X_features,
    y_labels,
    test_size=0.2,
    random_state=0
)

print(y_train.value_counts() / y_train.count())
print(y_test.value_counts() / y_test.count())

X_tr, X_val, y_tr, y_val = train_test_split(
    X_train,
    y_train,
    test_size=0.3,
    random_state=0
)

from hyperopt import hp

xgb_search_space = {
    'max_depth':hp.quniform('max_depth', 5, 15, 1),                     ## 정수형 하이퍼 파라미터 => quniform 사용
    'min_child_weight':hp.quniform('min_child_weight', 1, 6, 1),        ## 정수형 하이퍼 파라미터 => quniform 사용
    'learning_rate':hp.uniform('learning_rate', 0.01, 0.2),
    'colsample_bytree':hp.uniform('colsample_bytree', 0.5, 0.95),
}
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

def objective_func(search_space):

  xgb_clf = XGBClassifier(
      n_estimators=100,
      max_depth=int(search_space['max_depth']),
      min_child_weight=int(search_space['min_child_weight']),
      learning_rate=search_space['learning_rate'],
      colsample_bytree=search_space['colsample_bytree'],
      )


  roc_auc_list = []

  kf = KFold(n_splits=3)

  for tr_index, val_index in kf.split(X_train):

    X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
    X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]

    xgb_clf.fit(
        X_tr, y_tr,
        early_stopping_rounds=30,
        eval_metric='auc',
        eval_set=[(X_tr, y_tr), (X_val, y_val)]
    )

    score = roc_auc_score(
        y_val,
        xgb_clf.predict_proba(X_val)[:, 1]
    )

    roc_auc_list.append(score)

  return (-1) * np.mean(roc_auc_list)


from hyperopt import fmin, tpe, Trials

trials = Trials()

best = fmin(
    fn=objective_func,
    space=xgb_search_space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials,
)

xgb_wrapper = XGBClassifier(
    n_estimators=500,
    learning_rate=round(best['learning_rate'], 5),
    max_depth=int(best['max_depth']),
    min_child_weight=int(best['min_child_weight']),
    colsample_bytree=round(best['colsample_bytree'], 5)
)

xgb_wrapper.fit(
    X_tr, y_tr,
    early_stoppin_rounds=100,
    eval_metric='auc',
    eval_set=[(X_tr, y_tr), (X_test, y_test)]
)

xgb_roc_score = roc_auc_score(
    y_test,
    xgb_wrapper.predict_proba(X_test)[:, 1]
)

xgb_roc_score

from xgboost import plot_importance
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(10, 8))
plot_importance(xgb_wrapper,
                ax=ax,
                max_num_features=20,
                height=0.4,
                )

from hyperopt import hp

lgbm_search_space = {
    'num_leaves':hp.quniform('num_leaves', 32, 64, 1),
    'max_depth':hp.quniform('max_depth', 100, 169, 1),
    'min_child_weight':hp.quniform('min_child_weight', 60, 100, 1),
    'learning_rate':hp.uniform('learning_rate', 0.01, 0.2),
    'subsample':hp.uniform('subsample', 0.7, 1),
}


from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

def objective_func(search_space):

  lgbm_clf = LGBMClassifier(
      n_estimators=100,
      max_depth=int(search_space['max_depth']),
      min_child_weight=int(search_space['min_child_weight']),
      learning_rate=search_space['learning_rate'],
      subsample=search_space['subsample'],
      num_leaves=int(search_space['num_leaves']),
      )

  roc_auc_list = []

  kf = KFold(n_splits=3)

  for tr_index, val_index in kf.split(X_train):

    X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
    X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]

    lgbm_clf.fit(
        X_tr, y_tr,
        early_stopping_rounds=30,
        eval_metric='auc',
        eval_set=[(X_tr, y_tr), (X_val, y_val)]
    )

    score = roc_auc_score(
        y_val,
        lgbm_clf.predict_proba(X_val)[:, 1]
    )

    roc_auc_list.append(score)

  return (-1) * np.mean(roc_auc_list)


from hyperopt import fmin, tpe, Trials

trials = Trials()

best = fmin(
    fn=objective_func,
    space=lgbm_search_space,
    algo=tpe.suggest,
    max_evals=30,
    trials=trials,
)

lgbm_wrapper = LGBMClassifier(
    n_estimators=500,
    num_leaves=int(best['num_leaves']),
    learning_rate=round(best['learning_rate'], 5),
    max_depth=int(best['max_depth']),
    min_child_weight=int(best['min_child_weight']),
    subsample=round(best['subsample'], 5)
)

lgbm_wrapper.fit(
    X_tr, y_tr,
    early_stopping_rounds=100,
    eval_metric='auc',
    eval_set=[(X_tr, y_tr), (X_test, y_test)]
)

lgbm_roc_score = roc_auc_score(
    y_test,
    lgbm_wrapper.predict_proba(X_test)[:, 1]
)

lgbm_roc_score